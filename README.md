# LLM Zoomcamp end-to-end RAG application


<img src="images/asktheragbuddy_2.png" alt="Example Image" style="width:100%; height:100%;" />

## Try out AskTheraRAGBuddy [[here]](https://asktheraragbuddy-e7eagmgje3gcf3db.uksouth-01.azurewebsites.net/)


<img src="images/asktheragbuddy_1.png" alt="Example Image" style="width:60%; height:60%;" />


# Project Overview 

The primary goal of this project is to develop a cloud-based, end-to-end Retrieval-Augmented Generation (RAG) application deployed on the Microsoft Azure platform. The application aims to retrieve relevant mental health information from a knowledge base, which integrates data from Counselchat.com and the Mental Health Dataset provided by JIS College of Engineering. The objective is to generate accurate and contextually appropriate responses using a large language model (LLM) enhanced by RAG techniques.

## Project Scope

The project encompasses the following key steps:
1. **Dataset Selection and Preprocessing**: Identify and prepare the combined dataset from multiple sources to form a comprehensive knowledge base.
2. **Data Ingestion**: Ingest the preprocessed data into the knowledge base for efficient retrieval.
3. **Knowledge Base Querying**: Enable querying of the knowledge base to extract relevant information.
4. **Prompt Construction**: Develop appropriate prompts that effectively interact with the LLM.
5. **Response Generation**: Leverage the LLM to generate context-rich and accurate responses based on retrieved data.
6. **Performance Evaluation**: Assess the application’s performance to ensure response accuracy and relevance.
7. **User Interface (UI) Development**: Create a user-friendly interface for interaction.


## Final Application: AskTheraRAGBuddy

AskTheraRAGBuddy is a cloud-based web application developed using FastAPI, designed to help users answer mental health-related questions. The system is optimized for efficiency and ease of use, leveraging both the Mental Health Dataset and responses from qualified counselors on Counselchat.com.

This application demonstrates the practical utility of a Retrieval-Augmented Generation (RAG) system, allowing users to query the knowledge base and receive detailed, contextually relevant responses generated by a Large Language Model (LLM).

# Retrieval Evaluation

## Summary:
- **Consistency and diversity** are strong, showing the retrieval system functioned reliably and provided varied results across different queries.
- The **query variation score** could potentially be improved if we wanted more overlap between results of queries that are semantically similar.
- The **response time** is reasonable, with low variance.
- **Semantic coherence** is good, indicating that the retrieved documents align well in meaning across similar queries.

## Breakdown of results:
1. **Retrieval Consistency**: `1.0000`
   - This indicates the retrieval system is highly consistent—every time the same query is run, we get exactly the same documents. A score of 1.0 means 100% consistency, which is ideal for a system where reliability of results is important.

2. **Unique Documents Ratio**: `0.9833`
   - A unique documents ratio of 0.9833 suggests that about 98.33% of the documents returned are unique across the different queries. This is a good score and implies that the system is returning diverse results for different queries, minimizing duplication.

3. **Query Variation Score**: `0.5963`
   - A query variation score of 0.5963 indicates that variations of the same query (like "Signs of depression" vs. "Depression indicators") retrieve somewhat similar but not identical documents. A score closer to 1.0 would indicate high similarity, while a lower score would suggest significant variation in the results. In our case, this implies the system is retrieving somewhat related but not exactly the same documents for different wordings of the same query.

4. **Average Response Time**: `1.1586s` (±`0.0745s`)
   - The average response time of 1.1586 seconds is relatively good for a document retrieval system, especially if we're working with a large dataset or complex embeddings. The standard deviation of 0.0745 seconds suggests that the retrieval time is consistent, with little variance between requests.

5. **Semantic Coherence**: `0.8127`
   - A semantic coherence score of 0.8127 indicates that the documents returned across queries are fairly coherent in terms of their content and meaning. A score above 0.8 is typically considered strong, meaning that the system is returning semantically related content for similar queries.


## RAG Evaluation

## Acknowledgments

A big thank you to [Alexey Grigorev](https://github.com/alexeygrigorev) and his team for putting together this excellent course, and for making it available to all, free of charge. Creating such a course requires a considerable amount of time and effort, and I'm appreciative of these efforts. I'm very pleased to have been a part of the 2024 cohort of the [DataTalks.Club LLM Zoomcamp](https://github.com/DataTalksClub/llm-zoomcamp). Thank you all!






 

