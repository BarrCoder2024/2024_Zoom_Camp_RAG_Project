{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved C:\\\\Users\\\\matth\\\\My_Projects\\\\2024_Zoom_Camp_RAG_Project\\\\data\\mentalhealth_data_CLEAN_part1.json\n",
      "Saved C:\\\\Users\\\\matth\\\\My_Projects\\\\2024_Zoom_Camp_RAG_Project\\\\data\\mentalhealth_data_CLEAN_part2.json\n",
      "Saved C:\\\\Users\\\\matth\\\\My_Projects\\\\2024_Zoom_Camp_RAG_Project\\\\data\\mentalhealth_data_CLEAN_part3.json\n",
      "Saved C:\\\\Users\\\\matth\\\\My_Projects\\\\2024_Zoom_Camp_RAG_Project\\\\data\\mentalhealth_data_CLEAN_part4.json\n",
      "Saved C:\\\\Users\\\\matth\\\\My_Projects\\\\2024_Zoom_Camp_RAG_Project\\\\data\\mentalhealth_data_CLEAN_part5.json\n",
      "Saved C:\\\\Users\\\\matth\\\\My_Projects\\\\2024_Zoom_Camp_RAG_Project\\\\data\\mentalhealth_data_CLEAN_part6.json\n",
      "Saved C:\\\\Users\\\\matth\\\\My_Projects\\\\2024_Zoom_Camp_RAG_Project\\\\data\\mentalhealth_data_CLEAN_part7.json\n",
      "Saved C:\\\\Users\\\\matth\\\\My_Projects\\\\2024_Zoom_Camp_RAG_Project\\\\data\\mentalhealth_data_CLEAN_part8.json\n",
      "Saved C:\\\\Users\\\\matth\\\\My_Projects\\\\2024_Zoom_Camp_RAG_Project\\\\data\\mentalhealth_data_CLEAN_part9.json\n",
      "Saved C:\\\\Users\\\\matth\\\\My_Projects\\\\2024_Zoom_Camp_RAG_Project\\\\data\\mentalhealth_data_CLEAN_part10.json\n",
      "Saved C:\\\\Users\\\\matth\\\\My_Projects\\\\2024_Zoom_Camp_RAG_Project\\\\data\\mentalhealth_data_CLEAN_part11.json\n",
      "Completed splitting the files into 10 or so parts.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from dotenv import load_dotenv \n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv() \n",
    "DATASET_PATH = os.getenv('DATASET_PATH')  # Get the DATASET_PATH stored in .env file\n",
    "\n",
    "# Define file paths\n",
    "source_data_file = 'mentalhealth_data_ORIGINAL.json'\n",
    "source_data = os.path.join(DATASET_PATH, 'source_data', source_data_file)\n",
    "\n",
    "# Define the total number of parts (files) to split into\n",
    "NUM_FILES = 10\n",
    "\n",
    "# List of topics that should have \"Mental Health Dataset\" as the source\n",
    "mental_health_fact_topics = [\n",
    "    \"mental-health-fact\", \"fact-1\", \"fact-2\", \"fact-3\", \"fact-5\", \"fact-6\", \"fact-7\", \n",
    "    \"fact-8\", \"fact-9\", \"fact-10\", \"fact-11\", \"fact-12\", \"fact-13\", \"fact-14\", \n",
    "    \"fact-15\", \"fact-16\", \"fact-17\", \"fact-18\", \"fact-19\", \"fact-20\", \"fact-21\", \n",
    "    \"fact-22\", \"fact-23\", \"fact-24\", \"fact-25\", \"fact-26\", \"fact-27\", \"fact-28\", \n",
    "    \"fact-29\", \"fact-30\", \"fact-31\", \"fact-32\"\n",
    "]\n",
    "\n",
    "# Load JSON data\n",
    "def load_json_data(filepath):\n",
    "    with open(filepath, 'r') as infile:\n",
    "        return json.load(infile)\n",
    "\n",
    "# Function to replace \"Pandora\" with \"AskTheraRAGBuddy\", but keep \"Pandora's box\"\n",
    "def replace_pandora(text):\n",
    "    return text.replace(\"Pandora\", \"AskTheraRAGBuddy\").replace(\"AskTheraRAGBuddy's box\", \"Pandora's box\")\n",
    "\n",
    "# Function to transform an entry with patterns and responses\n",
    "def transform_entry(pattern, topic, source, responses, line_counter):\n",
    "    return {\n",
    "        \"question_id\": f\"mh_{line_counter}\",\n",
    "        \"topic\": topic,\n",
    "        \"question_title\": replace_pandora(pattern),\n",
    "        \"question_full\": replace_pandora(pattern),\n",
    "        \"source\": source,\n",
    "        \"answers\": [{\"answer\": replace_pandora(resp)} for resp in responses]  # Handle both cases\n",
    "    }\n",
    "\n",
    "# Function to write a chunk of data to a file\n",
    "def write_file(data_chunk, file_counter, output_dir):\n",
    "    file_name = os.path.join(output_dir, f\"mentalhealth_data_part{file_counter}.json\")\n",
    "    with open(file_name, 'w') as outfile:\n",
    "        json.dump(data_chunk, outfile, indent=4)\n",
    "    print(f\"Saved {file_name}\")\n",
    "\n",
    "# Function to count total lines\n",
    "def count_total_lines(intents):\n",
    "    total_lines = 0\n",
    "    for entry in intents:\n",
    "        for pattern in entry.get('patterns', []):\n",
    "            transformed_entry = {\n",
    "                \"question_id\": f\"mh_{total_lines}\",\n",
    "                \"topic\": entry.get('tag', 'Unknown'),\n",
    "                \"question_title\": replace_pandora(pattern),\n",
    "                \"question_full\": replace_pandora(pattern),\n",
    "                \"source\": \"Mental Health Dataset\" if entry.get('tag', 'Unknown') in mental_health_fact_topics else \"AskTheraRAGBuddy\",\n",
    "                \"answers\": [{\"answer\": replace_pandora(resp)} for resp in entry.get('responses', [])]\n",
    "            }\n",
    "            total_lines += len(json.dumps(transformed_entry, indent=4).splitlines())\n",
    "    return total_lines\n",
    "\n",
    "# Main function to split the data into multiple parts\n",
    "def process_and_split_data(intents, output_dir, lines_per_file, mental_health_fact_topics):\n",
    "    file_counter = 1\n",
    "    line_counter = 0\n",
    "    output_data = []\n",
    "\n",
    "    for entry in intents:\n",
    "        topic = entry.get('tag', 'Unknown')\n",
    "        source = \"Mental Health Dataset\" if topic in mental_health_fact_topics else \"AskTheraRAGBuddy\"\n",
    "        \n",
    "        for pattern in entry.get('patterns', []):\n",
    "            responses = entry.get('responses', [])\n",
    "            if not responses:\n",
    "                responses = entry.get('response', [])\n",
    "            \n",
    "            transformed_entry = transform_entry(pattern, topic, source, responses, line_counter)\n",
    "            output_data.append(transformed_entry)\n",
    "            \n",
    "            # Count the lines for this entry and add to total\n",
    "            line_counter += len(json.dumps(transformed_entry, indent=4).splitlines())\n",
    "            \n",
    "            # If we've hit the target lines per file, write out the file and reset\n",
    "            if line_counter >= lines_per_file:\n",
    "                write_file(output_data, file_counter, output_dir)\n",
    "                file_counter += 1\n",
    "                output_data = []  # Reset the data chunk\n",
    "                line_counter = 0  # Reset the line counter\n",
    "\n",
    "    # Write any remaining data to the final file\n",
    "    if output_data:\n",
    "        write_file(output_data, file_counter, output_dir)\n",
    "\n",
    "# Main Execution Flow\n",
    "if __name__ == \"__main__\":\n",
    "    # Load the data\n",
    "    data = load_json_data(source_data)\n",
    "    intents = data[\"intents\"]  # Access the \"intents\" key\n",
    "\n",
    "    # Count the total lines\n",
    "    total_lines = count_total_lines(intents)\n",
    "\n",
    "    # Calculate lines per file\n",
    "    lines_per_file = total_lines // NUM_FILES\n",
    "\n",
    "    # Process and split the data\n",
    "    process_and_split_data(intents, DATASET_PATH, lines_per_file, mental_health_fact_topics)\n",
    "\n",
    "    print(f\"Completed splitting the files into {NUM_FILES} or so parts.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import re\n",
    "from dotenv import load_dotenv "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the paths\n",
    "\n",
    "load_dotenv() # Load environment variables from .env file\n",
    "DATASET_PATH = os.getenv('DATASET_PATH') # Get the DATASET_PATH stored in .env file\n",
    "\n",
    "# Define file paths\n",
    "source_data_file = 'mentalhealth_data_ORIGINAL.json'\n",
    "source_data = os.path.join(DATASET_PATH, 'source_data', source_data_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the total number of parts (files) to split into\n",
    "NUM_FILES = 10\n",
    "\n",
    "# List of topics that should have \"Mental Health Dataset\" as the source\n",
    "mental_health_fact_topics = [\n",
    "    \"mental-health-fact\", \"fact-1\", \"fact-2\", \"fact-3\", \"fact-5\", \"fact-6\", \"fact-7\", \n",
    "    \"fact-8\", \"fact-9\", \"fact-10\", \"fact-11\", \"fact-12\", \"fact-13\", \"fact-14\", \n",
    "    \"fact-15\", \"fact-16\", \"fact-17\", \"fact-18\", \"fact-19\", \"fact-20\", \"fact-21\", \n",
    "    \"fact-22\", \"fact-23\", \"fact-24\", \"fact-25\", \"fact-26\", \"fact-27\", \"fact-28\", \n",
    "    \"fact-29\", \"fact-30\", \"fact-31\", \"fact-32\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved mentalhealth_data_CLEAN_part1.json\n",
      "Saved mentalhealth_data_CLEAN_part2.json\n",
      "Saved mentalhealth_data_CLEAN_part3.json\n",
      "Saved mentalhealth_data_CLEAN_part4.json\n",
      "Saved mentalhealth_data_CLEAN_part5.json\n",
      "Saved mentalhealth_data_CLEAN_part6.json\n",
      "Saved mentalhealth_data_CLEAN_part7.json\n",
      "Saved mentalhealth_data_CLEAN_part8.json\n",
      "Saved mentalhealth_data_CLEAN_part9.json\n",
      "Saved mentalhealth_data_CLEAN_part10.json\n",
      "Saved mentalhealth_data_CLEAN_part11.json\n",
      "Completed splitting the files into exactly 10 parts.\n"
     ]
    }
   ],
   "source": [
    "# Load the source JSON file\n",
    "with open(source_data, 'r') as infile:\n",
    "    data = json.load(infile)  # Load JSON data\n",
    "    intents = data[\"intents\"]  # Access the \"intents\" key directly\n",
    "\n",
    "# Function to replace \"Pandora\" with \"AskTheraRAGBuddy\", but keep \"Pandora's box\"\n",
    "def replace_pandora(text):\n",
    "    return text.replace(\"Pandora\", \"AskTheraRAGBuddy\").replace(\"AskTheraRAGBuddy's box\", \"Pandora's box\")\n",
    "\n",
    "# Count total lines first\n",
    "total_lines = 0\n",
    "for entry in intents:\n",
    "    for pattern in entry.get('patterns', []):\n",
    "        # Build a temporary transformed entry to count its lines\n",
    "        transformed_entry = {\n",
    "            \"question_id\": f\"mh_{total_lines}\",\n",
    "            \"topic\": entry.get('tag', 'Unknown'),\n",
    "            \"question_title\": replace_pandora(pattern),\n",
    "            \"question_full\": replace_pandora(pattern),\n",
    "            \"source\": \"Mental Health Dataset\" if entry.get('tag', 'Unknown') in mental_health_fact_topics else \"AskTheraRAGBuddy\",\n",
    "            \"answers\": [{\"answer\": replace_pandora(resp)} for resp in entry.get('responses', [])]\n",
    "        }\n",
    "        total_lines += len(json.dumps(transformed_entry, indent=4).splitlines())\n",
    "\n",
    "# Calculate lines per file\n",
    "lines_per_file = total_lines // NUM_FILES\n",
    "\n",
    "# Now split the file\n",
    "file_counter = 1\n",
    "line_counter = 0\n",
    "output_data = []\n",
    "\n",
    "# Write out the current chunk of data to a file\n",
    "def write_file(data_chunk, file_counter):\n",
    "    file_name = f\"mentalhealth_data_CLEAN_part{file_counter}.json\"\n",
    "    with open(file_name, 'w') as outfile:\n",
    "        json.dump(data_chunk, outfile, indent=4)\n",
    "    print(f\"Saved {file_name}\")\n",
    "\n",
    "# Loop through the JSON data and split based on the calculated lines per file\n",
    "for entry in intents:\n",
    "    topic = entry.get('tag', 'Unknown')  # Default to 'Unknown' if 'tag' is missing\n",
    "\n",
    "    # Modify the source based on the topic\n",
    "    source = \"Mental Health Dataset\" if topic in mental_health_fact_topics else \"AskTheraRAGBuddy\"\n",
    "\n",
    "    # Process each pattern as a separate question\n",
    "    \n",
    "    for pattern in entry.get('patterns', []):\n",
    "        # Build the new entry with the updated \"Pandora\" replacement\n",
    "        responses = entry.get('responses', [])  # Check for \"responses\"\n",
    "        \n",
    "        if not responses:\n",
    "            responses = entry.get('response', [])  # If \"responses\" is empty, check \"response\"\n",
    "    \n",
    "        transformed_entry = {\n",
    "            \"question_id\": f\"mh_{line_counter}\",\n",
    "            \"topic\": topic,\n",
    "            \"question_title\": replace_pandora(pattern),\n",
    "            \"question_full\": replace_pandora(pattern),\n",
    "            \"source\": source,\n",
    "            \"answers\": [{\"answer\": replace_pandora(resp)} for resp in responses]  # Handle both cases\n",
    "        }\n",
    "        output_data.append(transformed_entry)\n",
    "        \n",
    "        # Count the lines for this entry and add to total\n",
    "        line_counter += len(json.dumps(transformed_entry, indent=4).splitlines())\n",
    "\n",
    "        # If we've hit the target lines per file, write out the file and reset\n",
    "        if line_counter >= lines_per_file:\n",
    "            write_file(output_data, file_counter)\n",
    "            file_counter += 1\n",
    "            output_data = []  # Reset the data chunk\n",
    "            line_counter = 0  # Reset the line counter\n",
    "\n",
    "# Write any remaining data to the final file\n",
    "if output_data:\n",
    "    write_file(output_data, file_counter)\n",
    "\n",
    "print(\"Completed splitting the files into exactly 10 parts.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
