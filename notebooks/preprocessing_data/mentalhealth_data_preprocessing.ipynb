{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved C:\\\\Users\\\\matth\\\\My_Projects\\\\2024_Zoom_Camp_RAG_Project\\\\data\\mentalhealth_data_part1.json\n",
      "Saved C:\\\\Users\\\\matth\\\\My_Projects\\\\2024_Zoom_Camp_RAG_Project\\\\data\\mentalhealth_data_part2.json\n",
      "Saved C:\\\\Users\\\\matth\\\\My_Projects\\\\2024_Zoom_Camp_RAG_Project\\\\data\\mentalhealth_data_part3.json\n",
      "Saved C:\\\\Users\\\\matth\\\\My_Projects\\\\2024_Zoom_Camp_RAG_Project\\\\data\\mentalhealth_data_part4.json\n",
      "Saved C:\\\\Users\\\\matth\\\\My_Projects\\\\2024_Zoom_Camp_RAG_Project\\\\data\\mentalhealth_data_part5.json\n",
      "Saved C:\\\\Users\\\\matth\\\\My_Projects\\\\2024_Zoom_Camp_RAG_Project\\\\data\\mentalhealth_data_part6.json\n",
      "Saved C:\\\\Users\\\\matth\\\\My_Projects\\\\2024_Zoom_Camp_RAG_Project\\\\data\\mentalhealth_data_part7.json\n",
      "Saved C:\\\\Users\\\\matth\\\\My_Projects\\\\2024_Zoom_Camp_RAG_Project\\\\data\\mentalhealth_data_part8.json\n",
      "Saved C:\\\\Users\\\\matth\\\\My_Projects\\\\2024_Zoom_Camp_RAG_Project\\\\data\\mentalhealth_data_part9.json\n",
      "Saved C:\\\\Users\\\\matth\\\\My_Projects\\\\2024_Zoom_Camp_RAG_Project\\\\data\\mentalhealth_data_part10.json\n",
      "Completed splitting the files into parts with approximately 380000 lines each.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from dotenv import load_dotenv \n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv() \n",
    "DATASET_PATH = os.getenv('DATASET_PATH')  # Get the DATASET_PATH stored in .env file\n",
    "\n",
    "# Define file paths\n",
    "source_data_file = 'mentalhealth_data_ORIGINAL.json'\n",
    "source_data = os.path.join(DATASET_PATH, 'source_data', source_data_file)\n",
    "\n",
    "# Define the total number of parts (files) to split into (approximately)\n",
    "NUM_FILES = 10\n",
    "TARGET_LINES_PER_FILE = 380000  # Approximate number of lines per file\n",
    "\n",
    "# List of topics that should have \"Mental Health Dataset\" as the source\n",
    "mental_health_fact_topics = [\n",
    "    \"mental-health-fact\", \"fact-1\", \"fact-2\", \"fact-3\", \"fact-5\", \"fact-6\", \"fact-7\", \n",
    "    \"fact-8\", \"fact-9\", \"fact-10\", \"fact-11\", \"fact-12\", \"fact-13\", \"fact-14\", \n",
    "    \"fact-15\", \"fact-16\", \"fact-17\", \"fact-18\", \"fact-19\", \"fact-20\", \"fact-21\", \n",
    "    \"fact-22\", \"fact-23\", \"fact-24\", \"fact-25\", \"fact-26\", \"fact-27\", \"fact-28\", \n",
    "    \"fact-29\", \"fact-30\", \"fact-31\", \"fact-32\"\n",
    "]\n",
    "\n",
    "# Load JSON data\n",
    "def load_json_data(filepath):\n",
    "    with open(filepath, 'r') as infile:\n",
    "        return json.load(infile)\n",
    "\n",
    "# Function to replace \"Pandora\" with \"AskTheraRAGBuddy\", but keep \"Pandora's box\"\n",
    "def replace_pandora(text):\n",
    "    return text.replace(\"Pandora\", \"AskTheraRAGBuddy\").replace(\"AskTheraRAGBuddy's box\", \"Pandora's box\")\n",
    "\n",
    "# Function to transform an entry with patterns and responses\n",
    "def transform_entry(pattern, topic, source, responses, line_counter):\n",
    "    return {\n",
    "        \"question_id\": f\"mh_{line_counter}\",\n",
    "        \"topic\": topic,\n",
    "        \"question_title\": replace_pandora(pattern),\n",
    "        \"question_full\": replace_pandora(pattern),\n",
    "        \"answers\": [{\"answer\": replace_pandora(resp), \"source\": source} for resp in responses]  # Ensure each answer has the source field\n",
    "    }\n",
    "\n",
    "# Function to write a chunk of data to a file\n",
    "def write_file(data_chunk, file_counter, output_dir):\n",
    "    file_name = os.path.join(output_dir, f\"mentalhealth_data_part{file_counter}.json\")\n",
    "    with open(file_name, 'w') as outfile:\n",
    "        json.dump(data_chunk, outfile, indent=4)\n",
    "    print(f\"Saved {file_name}\")\n",
    "\n",
    "# Function to count lines of an entry\n",
    "def count_lines(entry):\n",
    "    return len(json.dumps(entry, indent=4).splitlines())\n",
    "\n",
    "# Main function to split the data into multiple parts based on line count\n",
    "def process_and_split_data(intents, output_dir, target_lines_per_file, mental_health_fact_topics):\n",
    "    file_counter = 1\n",
    "    global_line_counter = 0  # Use a global line counter across files\n",
    "    current_file_lines = 0   # To track the number of lines in the current file\n",
    "    output_data = []\n",
    "\n",
    "    for entry in intents:\n",
    "        topic = entry.get('tag', 'Unknown')\n",
    "        source = \"Mental Health Dataset\" if topic in mental_health_fact_topics else \"AskTheraRAGBuddy\"\n",
    "        \n",
    "        for pattern in entry.get('patterns', []):\n",
    "            responses = entry.get('responses', [])\n",
    "            if not responses:\n",
    "                responses = entry.get('response', [])\n",
    "            \n",
    "            # Use the global line counter to ensure unique question_ids\n",
    "            transformed_entry = transform_entry(pattern, topic, source, responses, global_line_counter)\n",
    "            \n",
    "            # Count the lines of this entry\n",
    "            entry_lines = count_lines(transformed_entry)\n",
    "            \n",
    "            # Check if adding this entry would exceed the target lines for this file\n",
    "            if current_file_lines + entry_lines > target_lines_per_file:\n",
    "                # If so, write the current file and reset for the next file\n",
    "                write_file(output_data, file_counter, output_dir)\n",
    "                file_counter += 1\n",
    "                output_data = []  # Reset the data chunk\n",
    "                current_file_lines = 0  # Reset the line counter for the new file\n",
    "\n",
    "            # Add the entry to the current file\n",
    "            output_data.append(transformed_entry)\n",
    "            current_file_lines += entry_lines\n",
    "            global_line_counter += 1  # Increment global line counter to keep unique question_ids\n",
    "\n",
    "    # Write any remaining data to the final file\n",
    "    if output_data:\n",
    "        write_file(output_data, file_counter, output_dir)\n",
    "\n",
    "\n",
    "# Load the data\n",
    "data = load_json_data(source_data)\n",
    "intents = data[\"intents\"]  # Access the \"intents\" key\n",
    "\n",
    "# Process and split the data\n",
    "process_and_split_data(intents, DATASET_PATH, TARGET_LINES_PER_FILE, mental_health_fact_topics)\n",
    "\n",
    "print(f\"Completed splitting the files into parts with approximately {TARGET_LINES_PER_FILE} lines each.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
